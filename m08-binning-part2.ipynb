{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning part 2: create bins with `metabat2`\n",
    "\n",
    "Now, we continue the binning procedure with metabat.\n",
    "In this notebook, we will create the actual bins! \n",
    "We will need:\n",
    "* The scaffolds of the assembly, to bin.\n",
    "* The depth matrix we made with the jgi script\n",
    "* A new folder, to store the newly created bins.\n",
    "\n",
    "First, remember where the first two items listed above are. \n",
    "Use `ls` to confirm in the cell below.\n",
    "\n",
    "Scaffolds are located at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaffolds.fasta      scaffolds.fasta.bwt  scaffolds.fasta.sa\n",
      "scaffolds.fasta.amb  \u001b[0m\u001b[01;31mscaffolds.fasta.gz\u001b[0m\n",
      "scaffolds.fasta.ann  scaffolds.fasta.pac\n"
     ]
    }
   ],
   "source": [
    "ls data/assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depth matrix is located at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/depth_matrix.tab\n"
     ]
    }
   ],
   "source": [
    "ls data/depth_matrix.tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a new directory to store your bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir data/bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the help page, the command is `metabat2`\n",
    "\n",
    "Find out which options you have to use minimally, then make sure you tell MetaBAT to use one thread only!\n",
    "\n",
    "supply your depth matrix as the --abdFile. Short for AbundanceFile.\n",
    "\n",
    "> By default, metabat uses all threads. If all of us use all threads at the same time, the server will most certainly crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaBAT: Metagenome Binning based on Abundance and Tetranucleotide frequency (version 2:2.15 (Bioconda); 2020-01-04T21:10:40)\n",
      "by Don Kang (ddkang@lbl.gov), Feng Li, Jeff Froula, Rob Egan, and Zhong Wang (zhongwang@lbl.gov) \n",
      "\n",
      "Allowed options:\n",
      "  -h [ --help ]                     produce help message\n",
      "  -i [ --inFile ] arg               Contigs in (gzipped) fasta file format [Mandatory]\n",
      "  -o [ --outFile ] arg              Base file name and path for each bin. The default output is fasta format.\n",
      "                                    Use -l option to output only contig names [Mandatory].\n",
      "  -a [ --abdFile ] arg              A file having mean and variance of base coverage depth (tab delimited; \n",
      "                                    the first column should be contig names, and the first row will be \n",
      "                                    considered as the header and be skipped) [Optional].\n",
      "  -m [ --minContig ] arg (=2500)    Minimum size of a contig for binning (should be >=1500).\n",
      "  --maxP arg (=95)                  Percentage of 'good' contigs considered for binning decided by connection\n",
      "                                    among contigs. The greater, the more sensitive.\n",
      "  --minS arg (=60)                  Minimum score of a edge for binning (should be between 1 and 99). The \n",
      "                                    greater, the more specific.\n",
      "  --maxEdges arg (=200)             Maximum number of edges per node. The greater, the more sensitive.\n",
      "  --pTNF arg (=0)                   TNF probability cutoff for building TNF graph. Use it to skip the \n",
      "                                    preparation step. (0: auto).\n",
      "  --noAdd                           Turning off additional binning for lost or small contigs.\n",
      "  --cvExt                           When a coverage file without variance (from third party tools) is used \n",
      "                                    instead of abdFile from jgi_summarize_bam_contig_depths.\n",
      "  -x [ --minCV ] arg (=1)           Minimum mean coverage of a contig in each library for binning.\n",
      "  --minCVSum arg (=1)               Minimum total effective mean coverage of a contig (sum of depth over \n",
      "                                    minCV) for binning.\n",
      "  -s [ --minClsSize ] arg (=200000) Minimum size of a bin as the output.\n",
      "  -t [ --numThreads ] arg (=0)      Number of threads to use (0: use all cores).\n",
      "  -l [ --onlyLabel ]                Output only sequence labels as a list in a column without sequences.\n",
      "  --saveCls                         Save cluster memberships as a matrix format\n",
      "  --unbinned                        Generate [outFile].unbinned.fa file for unbinned contigs\n",
      "  --noBinOut                        No bin output. Usually combined with --saveCls to check only contig \n",
      "                                    memberships\n",
      "  --seed arg (=0)                   For exact reproducibility. (0: use random seed)\n",
      "  -d [ --debug ]                    Debug output\n",
      "  -v [ --verbose ]                  Verbose output\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metabat2 -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run metabat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaBAT 2 (2.15 (Bioconda)) using minContig 2500, minCV 1.0, minCVSum 1.0, maxP 95%, minS 60, maxEdges 200 and minClsSize 200000. with random seed=1647960494\n",
      "6 bins (25241689 bases in total) formed.\n"
     ]
    }
   ],
   "source": [
    "metabat2 -i ./data/assembly/scaffolds.fasta -o ./data/bins/bin -a ./data/depth_matrix.tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualisation\n",
    "Now we have our metagenome binned!\n",
    "Congratulations.\n",
    "Let's try to visualise this similarly as done in Binning part 1. \n",
    "We will use some command line tricks to get all data in a similar sheet.\n",
    "In this case, these are given to you already.\n",
    "However, please do try to reverse engineer the code and understand what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the cell below, we make an empty file with a header in which we will make our table. \n",
    "* Then we move to the folder in which we made the bins. \n",
    "* In this folder, we start a loop for each file that ends with `.fa`\n",
    "* For each `.fa` file, we extract the bin number and make a variable that we call `name`\n",
    "* Continuing in this iteration of the loop, we filter all fasta headers\n",
    "* directly after filtering, replace the fasta header sign '>' with the `name` variable defined earlier.\n",
    "* we end the loop and sort all resulting tables at once on the second column \n",
    "* after sorting, we append our newly made table to the 'binlist' file we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo -e 'bin\\tcontigName' > binlist\n",
    "cd ./data/bins/\n",
    "for f in *.fa\n",
    "do  name=$(echo $f | cut -d '.' -f 2)\n",
    "    grep '>' $f | sed \"s/^>/$name\\t/g\"\n",
    "done | sort -k2 -V >> ../../binlist\n",
    "cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check, how does the file look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\tcontigName\n",
      "4\tNODE_1_length_1935275_cov_24.6805_ID_23901540\n",
      "4\tNODE_2_length_1021023_cov_23.722_ID_28578184\n",
      "4\tNODE_3_length_872793_cov_26.0458_ID_35712810\n",
      "4\tNODE_4_length_853167_cov_25.0814_ID_23902252\n",
      "1\tNODE_5_length_723368_cov_22.0639_ID_32380821\n",
      "6\tNODE_6_length_592196_cov_27.8287_ID_32359880\n",
      "1\tNODE_8_length_486458_cov_22.6157_ID_32183008\n",
      "4\tNODE_9_length_472787_cov_23.2794_ID_23901976\n",
      "6\tNODE_13_length_350638_cov_27.7245_ID_23902421\n"
     ]
    }
   ],
   "source": [
    "head binlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these with the names we have in our depth_matrix.\n",
    "They must be exactly the same to join these two different tables into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contigName\n",
      "NODE_1_length_1935275_cov_24.6805_ID_23901540\n",
      "NODE_2_length_1021023_cov_23.722_ID_28578184\n",
      "NODE_3_length_872793_cov_26.0458_ID_35712810\n",
      "NODE_4_length_853167_cov_25.0814_ID_23902252\n",
      "NODE_5_length_723368_cov_22.0639_ID_32380821\n",
      "NODE_6_length_592196_cov_27.8287_ID_32359880\n",
      "NODE_7_length_571573_cov_24.8726_ID_32329659\n",
      "NODE_8_length_486458_cov_22.6157_ID_32183008\n",
      "NODE_9_length_472787_cov_23.2794_ID_23901976\n",
      "cut: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "cut -f 1  data/depth_matrix.tab | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the `join` command to join the two tables.\n",
    "There must be a shared field in both tables.\n",
    "In the first table, this is the second column `-1 2`, and in the second file, this is the first column `-2 1`. \n",
    "\n",
    "We then take both files and give them to the join command.\n",
    "However, since `join` is very picky in how files are sorted, we re-sort them on-the-fly like so ` <(sort -k2d ./somefile.txt)` (second column, sort as dictionary).\n",
    "\n",
    "Lastly, since both files have headers, we supply the `--header` option and save the result as a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "join -1 2 -2 1 <(sort -k2d ./binlist) <(sort -k1d data/depth_matrix.tab) --header | tr ' ' \"\\t\" > binned_depth_matrix.tab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Like we did before, download this resulting table `binned_depth_matrix.tab` and open it in excel.\n",
    "2. Sort the file by the bin number. \n",
    "3. Erase columns you do not want to visualise\n",
    "4. Use conditional formatting to visualise depth profiles over the different samples per bin. \n",
    "\n",
    "Does this colour pattern make more sense than it did before? Is there any outliers or mistakes you can spot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Example sheet](https://docs.google.com/spreadsheets/d/1Cdkl8dT75CETGUA_l52Gh8g8qtWd9vv4QAGtTe9HpPU/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin depth\n",
    "Can you determine the depth of the six bins from this table?\n",
    "Think about the difference between depth and coverage.\n",
    "You will likely need to make a pivot table in excel/LibreOffice/googledrive.\n",
    "\n",
    "One step further, can you determine the depth of each bin per sample we mapped!\n",
    "In other words, which bin is abundance in which sample type The L samples, or the P samples?\n",
    "\n",
    "The research this practical is based on focusses on microbes inside the leaves (L samples).\n",
    "Which bins would you advise me to study further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: vary binning signals\n",
    "\n",
    "If you'd like, you can try to vary input signals. \n",
    "For each variation, make sure you save the bins in a separate directory with a clear name.\n",
    "* First, you can try to run metabat2 without the depth matrix. How many bins do you get then?\n",
    "* You can edit the depth matrix to only contain samples from one type (E or P)\n",
    "* You can edit the depth matrix to only contain one replicate per sample type, or two replicates?\n",
    "\n",
    "To modify your depth matrix, have a look at the collumns present:\n",
    "```\n",
    "head -n 3 <<your depth matrix>>\n",
    "```\n",
    "\n",
    "You can select certain columns with the cut command.\n",
    "This example shows you how to select only one replicate of one sample type and save this as a separate depth matrix.\n",
    "\n",
    "```\n",
    "cut -f 1-3,10,11 data/depth_matrix > data/depth_matrix_P1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
